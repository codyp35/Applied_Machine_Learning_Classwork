{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ed08866a30>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Practice Coding a Basic Neural Network from Scratch\n",
    "### Using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Iteration 0 36614852.25483738\n",
      "Loss on Iteration 1 34446557.00240053\n",
      "Loss on Iteration 2 35411563.12685293\n",
      "Loss on Iteration 3 32748412.284549486\n",
      "Loss on Iteration 4 24667094.480086192\n",
      "Loss on Iteration 5 14839943.22611718\n",
      "Loss on Iteration 6 7783207.578095639\n",
      "Loss on Iteration 7 4054212.361418286\n",
      "Loss on Iteration 8 2354168.099359283\n",
      "Loss on Iteration 9 1570259.2990361953\n",
      "Loss on Iteration 10 1169946.6054409132\n",
      "Loss on Iteration 11 932800.0099768736\n",
      "Loss on Iteration 12 771615.5399626193\n",
      "Loss on Iteration 13 651540.6971891754\n",
      "Loss on Iteration 14 557042.071859596\n",
      "Loss on Iteration 15 480203.68728233234\n",
      "Loss on Iteration 16 416565.3621768869\n",
      "Loss on Iteration 17 363195.07092329406\n",
      "Loss on Iteration 18 318066.61236489256\n",
      "Loss on Iteration 19 279631.787398327\n",
      "Loss on Iteration 20 246763.24904952644\n",
      "Loss on Iteration 21 218514.003869233\n",
      "Loss on Iteration 22 194103.97949855437\n",
      "Loss on Iteration 23 172901.37484909364\n",
      "Loss on Iteration 24 154437.81442546973\n",
      "Loss on Iteration 25 138293.52395077838\n",
      "Loss on Iteration 26 124143.90490140492\n",
      "Loss on Iteration 27 111714.27563006835\n",
      "Loss on Iteration 28 100751.39863340449\n",
      "Loss on Iteration 29 91046.63265628194\n",
      "Loss on Iteration 30 82445.1627441761\n",
      "Loss on Iteration 31 74793.53964865246\n",
      "Loss on Iteration 32 67968.23151994524\n",
      "Loss on Iteration 33 61870.11731022176\n",
      "Loss on Iteration 34 56417.057480812524\n",
      "Loss on Iteration 35 51545.45247493944\n",
      "Loss on Iteration 36 47162.81306777959\n",
      "Loss on Iteration 37 43214.155762774724\n",
      "Loss on Iteration 38 39654.23146019455\n",
      "Loss on Iteration 39 36439.88150963298\n",
      "Loss on Iteration 40 33526.75715158092\n",
      "Loss on Iteration 41 30881.86377885458\n",
      "Loss on Iteration 42 28479.86233556799\n",
      "Loss on Iteration 43 26294.145595391412\n",
      "Loss on Iteration 44 24302.252804823427\n",
      "Loss on Iteration 45 22483.54805197123\n",
      "Loss on Iteration 46 20821.63812711461\n",
      "Loss on Iteration 47 19299.68266051351\n",
      "Loss on Iteration 48 17905.487131244416\n",
      "Loss on Iteration 49 16625.752851530422\n",
      "Loss on Iteration 50 15450.151574434825\n",
      "Loss on Iteration 51 14369.254388349393\n",
      "Loss on Iteration 52 13374.618744241026\n",
      "Loss on Iteration 53 12458.140215251778\n",
      "Loss on Iteration 54 11612.703853888897\n",
      "Loss on Iteration 55 10832.18178191376\n",
      "Loss on Iteration 56 10111.1381713739\n",
      "Loss on Iteration 57 9444.595327228773\n",
      "Loss on Iteration 58 8827.605965025594\n",
      "Loss on Iteration 59 8255.963163642009\n",
      "Loss on Iteration 60 7725.948201983559\n",
      "Loss on Iteration 61 7234.132957527589\n",
      "Loss on Iteration 62 6777.577458059678\n",
      "Loss on Iteration 63 6353.659868203417\n",
      "Loss on Iteration 64 5959.382537113569\n",
      "Loss on Iteration 65 5592.789526103827\n",
      "Loss on Iteration 66 5251.506690166914\n",
      "Loss on Iteration 67 4933.31498534443\n",
      "Loss on Iteration 68 4636.617225276685\n",
      "Loss on Iteration 69 4359.809842218858\n",
      "Loss on Iteration 70 4101.283241535364\n",
      "Loss on Iteration 71 3859.803827998763\n",
      "Loss on Iteration 72 3634.1597793852607\n",
      "Loss on Iteration 73 3423.1664161794897\n",
      "Loss on Iteration 74 3225.9732211921996\n",
      "Loss on Iteration 75 3041.552072443465\n",
      "Loss on Iteration 76 2868.84105984126\n",
      "Loss on Iteration 77 2706.900735998496\n",
      "Loss on Iteration 78 2555.0152190705153\n",
      "Loss on Iteration 79 2412.543454598052\n",
      "Loss on Iteration 80 2278.8350885814734\n",
      "Loss on Iteration 81 2153.2219320100926\n",
      "Loss on Iteration 82 2035.23179533045\n",
      "Loss on Iteration 83 1924.3369308674914\n",
      "Loss on Iteration 84 1820.1220562939484\n",
      "Loss on Iteration 85 1722.0287868962218\n",
      "Loss on Iteration 86 1629.7126777002284\n",
      "Loss on Iteration 87 1542.8019046939748\n",
      "Loss on Iteration 88 1460.987885001591\n",
      "Loss on Iteration 89 1383.9044425987754\n",
      "Loss on Iteration 90 1311.2394165937017\n",
      "Loss on Iteration 91 1242.7435382572999\n",
      "Loss on Iteration 92 1178.1617528230554\n",
      "Loss on Iteration 93 1117.2097449996168\n",
      "Loss on Iteration 94 1059.6962095599051\n",
      "Loss on Iteration 95 1005.4077192771215\n",
      "Loss on Iteration 96 954.1442973148103\n",
      "Loss on Iteration 97 905.7080307481881\n",
      "Loss on Iteration 98 859.9429171080935\n",
      "Loss on Iteration 99 816.6937133515482\n",
      "Loss on Iteration 100 775.8106435423322\n",
      "Loss on Iteration 101 737.1269401327143\n",
      "Loss on Iteration 102 700.5237239488308\n",
      "Loss on Iteration 103 665.8905870005336\n",
      "Loss on Iteration 104 633.1299287928009\n",
      "Loss on Iteration 105 602.1106382908233\n",
      "Loss on Iteration 106 572.7370956161649\n",
      "Loss on Iteration 107 544.9134036635153\n",
      "Loss on Iteration 108 518.555632001319\n",
      "Loss on Iteration 109 493.5687246742783\n",
      "Loss on Iteration 110 469.8698423829715\n",
      "Loss on Iteration 111 447.3996763797063\n",
      "Loss on Iteration 112 426.09256466215544\n",
      "Loss on Iteration 113 405.8723549741653\n",
      "Loss on Iteration 114 386.68483639794346\n",
      "Loss on Iteration 115 368.4704242738879\n",
      "Loss on Iteration 116 351.18373299997586\n",
      "Loss on Iteration 117 334.76857711322054\n",
      "Loss on Iteration 118 319.1857709165912\n",
      "Loss on Iteration 119 304.3946822585981\n",
      "Loss on Iteration 120 290.3442189873606\n",
      "Loss on Iteration 121 276.984416518949\n",
      "Loss on Iteration 122 264.28455807309433\n",
      "Loss on Iteration 123 252.2073872336083\n",
      "Loss on Iteration 124 240.72615397444935\n",
      "Loss on Iteration 125 229.80260010034908\n",
      "Loss on Iteration 126 219.40753266955312\n",
      "Loss on Iteration 127 209.51590868816274\n",
      "Loss on Iteration 128 200.1010299275262\n",
      "Loss on Iteration 129 191.13621573682985\n",
      "Loss on Iteration 130 182.60007018525832\n",
      "Loss on Iteration 131 174.46861693758365\n",
      "Loss on Iteration 132 166.72613298592452\n",
      "Loss on Iteration 133 159.34759169406328\n",
      "Loss on Iteration 134 152.31641755383419\n",
      "Loss on Iteration 135 145.61760751985955\n",
      "Loss on Iteration 136 139.2325077613507\n",
      "Loss on Iteration 137 133.14310884427664\n",
      "Loss on Iteration 138 127.33604068711463\n",
      "Loss on Iteration 139 121.79760296622669\n",
      "Loss on Iteration 140 116.5169113526976\n",
      "Loss on Iteration 141 111.47610063084642\n",
      "Loss on Iteration 142 106.6669035667176\n",
      "Loss on Iteration 143 102.07740312194974\n",
      "Loss on Iteration 144 97.69788488414792\n",
      "Loss on Iteration 145 93.51576085876327\n",
      "Loss on Iteration 146 89.522350343527\n",
      "Loss on Iteration 147 85.70920662648177\n",
      "Loss on Iteration 148 82.06819941157437\n",
      "Loss on Iteration 149 78.58923637759872\n",
      "Loss on Iteration 150 75.26572385876055\n",
      "Loss on Iteration 151 72.09055140375978\n",
      "Loss on Iteration 152 69.05754258269509\n",
      "Loss on Iteration 153 66.15744203034453\n",
      "Loss on Iteration 154 63.38537612237459\n",
      "Loss on Iteration 155 60.735252157805746\n",
      "Loss on Iteration 156 58.20204537947046\n",
      "Loss on Iteration 157 55.77898582989266\n",
      "Loss on Iteration 158 53.46209580463758\n",
      "Loss on Iteration 159 51.2458983601674\n",
      "Loss on Iteration 160 49.12616697289902\n",
      "Loss on Iteration 161 47.09768328871135\n",
      "Loss on Iteration 162 45.1573148840619\n",
      "Loss on Iteration 163 43.300317970247136\n",
      "Loss on Iteration 164 41.52353756312358\n",
      "Loss on Iteration 165 39.82215961542586\n",
      "Loss on Iteration 166 38.19364008299892\n",
      "Loss on Iteration 167 36.6347499969455\n",
      "Loss on Iteration 168 35.14236904822438\n",
      "Loss on Iteration 169 33.71307799944964\n",
      "Loss on Iteration 170 32.34437066984948\n",
      "Loss on Iteration 171 31.033692893361284\n",
      "Loss on Iteration 172 29.778234687683387\n",
      "Loss on Iteration 173 28.575269550574085\n",
      "Loss on Iteration 174 27.422916440040325\n",
      "Loss on Iteration 175 26.319022909221296\n",
      "Loss on Iteration 176 25.261105471120093\n",
      "Loss on Iteration 177 24.24721281640084\n",
      "Loss on Iteration 178 23.275628998173232\n",
      "Loss on Iteration 179 22.344457173457375\n",
      "Loss on Iteration 180 21.451691552971134\n",
      "Loss on Iteration 181 20.59587357620576\n",
      "Loss on Iteration 182 19.77567219960732\n",
      "Loss on Iteration 183 18.989188002234204\n",
      "Loss on Iteration 184 18.23490299467784\n",
      "Loss on Iteration 185 17.511583395533144\n",
      "Loss on Iteration 186 16.817871262254403\n",
      "Loss on Iteration 187 16.15258680616643\n",
      "Loss on Iteration 188 15.514291972547529\n",
      "Loss on Iteration 189 14.902057514362223\n",
      "Loss on Iteration 190 14.314842013203268\n",
      "Loss on Iteration 191 13.751328872233238\n",
      "Loss on Iteration 192 13.210744790429013\n",
      "Loss on Iteration 193 12.69210806948984\n",
      "Loss on Iteration 194 12.19440097644238\n",
      "Loss on Iteration 195 11.716631528522718\n",
      "Loss on Iteration 196 11.25813471121776\n",
      "Loss on Iteration 197 10.818147122035029\n",
      "Loss on Iteration 198 10.395789423771287\n",
      "Loss on Iteration 199 9.990340213591592\n",
      "Loss on Iteration 200 9.601110094928535\n",
      "Loss on Iteration 201 9.227550779777893\n",
      "Loss on Iteration 202 8.868799519782982\n",
      "Loss on Iteration 203 8.524327898207286\n",
      "Loss on Iteration 204 8.193645933791872\n",
      "Loss on Iteration 205 7.876100620041759\n",
      "Loss on Iteration 206 7.571083539675863\n",
      "Loss on Iteration 207 7.278200728756581\n",
      "Loss on Iteration 208 6.9969620082381425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Iteration 209 6.72679772610825\n",
      "Loss on Iteration 210 6.467259853581913\n",
      "Loss on Iteration 211 6.217987401849477\n",
      "Loss on Iteration 212 5.978556651277766\n",
      "Loss on Iteration 213 5.748493878395159\n",
      "Loss on Iteration 214 5.527486664400188\n",
      "Loss on Iteration 215 5.315172448796422\n",
      "Loss on Iteration 216 5.1111821674786455\n",
      "Loss on Iteration 217 4.915161686125597\n",
      "Loss on Iteration 218 4.726813332869179\n",
      "Loss on Iteration 219 4.54585691340168\n",
      "Loss on Iteration 220 4.37195208497354\n",
      "Loss on Iteration 221 4.20479495855287\n",
      "Loss on Iteration 222 4.044165159589211\n",
      "Loss on Iteration 223 3.889775651329493\n",
      "Loss on Iteration 224 3.741381724320403\n",
      "Loss on Iteration 225 3.5987448474502077\n",
      "Loss on Iteration 226 3.4616684530239796\n",
      "Loss on Iteration 227 3.3298818442624927\n",
      "Loss on Iteration 228 3.203195261760099\n",
      "Loss on Iteration 229 3.0814344046378177\n",
      "Loss on Iteration 230 2.964365005024062\n",
      "Loss on Iteration 231 2.851800444592275\n",
      "Loss on Iteration 232 2.743582101596413\n",
      "Loss on Iteration 233 2.639561873974283\n",
      "Loss on Iteration 234 2.5395285483831724\n",
      "Loss on Iteration 235 2.4433343382537007\n",
      "Loss on Iteration 236 2.3508534288808374\n",
      "Loss on Iteration 237 2.261921036969732\n",
      "Loss on Iteration 238 2.176391073701241\n",
      "Loss on Iteration 239 2.094148615403708\n",
      "Loss on Iteration 240 2.0150633405990916\n",
      "Loss on Iteration 241 1.9389951377636567\n",
      "Loss on Iteration 242 1.8658370135905387\n",
      "Loss on Iteration 243 1.7954991259073232\n",
      "Loss on Iteration 244 1.7278275726935504\n",
      "Loss on Iteration 245 1.662738462395592\n",
      "Loss on Iteration 246 1.600140609875222\n",
      "Loss on Iteration 247 1.5399347467927873\n",
      "Loss on Iteration 248 1.482010592170671\n",
      "Loss on Iteration 249 1.4262922521715433\n",
      "Loss on Iteration 250 1.372707435660859\n",
      "Loss on Iteration 251 1.321145866570094\n",
      "Loss on Iteration 252 1.2715443157305077\n",
      "Loss on Iteration 253 1.2238362393606987\n",
      "Loss on Iteration 254 1.1779331380602764\n",
      "Loss on Iteration 255 1.1337685730503824\n",
      "Loss on Iteration 256 1.0912816184663885\n",
      "Loss on Iteration 257 1.0504109920977922\n",
      "Loss on Iteration 258 1.0110810816224782\n",
      "Loss on Iteration 259 0.9732386683113461\n",
      "Loss on Iteration 260 0.9368363970760548\n",
      "Loss on Iteration 261 0.9018070767162173\n",
      "Loss on Iteration 262 0.8680960096457122\n",
      "Loss on Iteration 263 0.8356618931199863\n",
      "Loss on Iteration 264 0.804447829821504\n",
      "Loss on Iteration 265 0.7744082314331497\n",
      "Loss on Iteration 266 0.7455061531346789\n",
      "Loss on Iteration 267 0.7176916450604429\n",
      "Loss on Iteration 268 0.6909225476992008\n",
      "Loss on Iteration 269 0.6651644855944209\n",
      "Loss on Iteration 270 0.6403760536240385\n",
      "Loss on Iteration 271 0.6165158546972875\n",
      "Loss on Iteration 272 0.5935535616491028\n",
      "Loss on Iteration 273 0.571460254385547\n",
      "Loss on Iteration 274 0.5501893888824906\n",
      "Loss on Iteration 275 0.5297180673355844\n",
      "Loss on Iteration 276 0.5100176737227979\n",
      "Loss on Iteration 277 0.49105337391702697\n",
      "Loss on Iteration 278 0.4727999704519428\n",
      "Loss on Iteration 279 0.4552326564820586\n",
      "Loss on Iteration 280 0.43832253596778087\n",
      "Loss on Iteration 281 0.4220448163785226\n",
      "Loss on Iteration 282 0.4063774334304536\n",
      "Loss on Iteration 283 0.3912965099014911\n",
      "Loss on Iteration 284 0.3767775918059497\n",
      "Loss on Iteration 285 0.36280214882571094\n",
      "Loss on Iteration 286 0.34935043046933845\n",
      "Loss on Iteration 287 0.3363991713244297\n",
      "Loss on Iteration 288 0.32393227782216727\n",
      "Loss on Iteration 289 0.3119337196684384\n",
      "Loss on Iteration 290 0.30038048703342984\n",
      "Loss on Iteration 291 0.2892568054071657\n",
      "Loss on Iteration 292 0.2785506004484132\n",
      "Loss on Iteration 293 0.2682414091693592\n",
      "Loss on Iteration 294 0.25831606894114034\n",
      "Loss on Iteration 295 0.24876206111351434\n",
      "Loss on Iteration 296 0.2395623122189659\n",
      "Loss on Iteration 297 0.23070477505938522\n",
      "Loss on Iteration 298 0.22217846892283594\n",
      "Loss on Iteration 299 0.21396793295188055\n",
      "Loss on Iteration 300 0.20606267466619332\n",
      "Loss on Iteration 301 0.19845243990673817\n",
      "Loss on Iteration 302 0.1911242531343305\n",
      "Loss on Iteration 303 0.1840679472691553\n",
      "Loss on Iteration 304 0.1772761943917678\n",
      "Loss on Iteration 305 0.17073473791802402\n",
      "Loss on Iteration 306 0.1644363101154448\n",
      "Loss on Iteration 307 0.15837198074974324\n",
      "Loss on Iteration 308 0.15253211999880262\n",
      "Loss on Iteration 309 0.14690825482504583\n",
      "Loss on Iteration 310 0.14149360274904133\n",
      "Loss on Iteration 311 0.1362794095718065\n",
      "Loss on Iteration 312 0.13125811419500827\n",
      "Loss on Iteration 313 0.12642347274380591\n",
      "Loss on Iteration 314 0.12176763885391559\n",
      "Loss on Iteration 315 0.1172837498698161\n",
      "Loss on Iteration 316 0.11296649796169958\n",
      "Loss on Iteration 317 0.10880875349990123\n",
      "Loss on Iteration 318 0.10480478471907645\n",
      "Loss on Iteration 319 0.10094961216196392\n",
      "Loss on Iteration 320 0.09723667717846127\n",
      "Loss on Iteration 321 0.09366038084785401\n",
      "Loss on Iteration 322 0.09021677585220983\n",
      "Loss on Iteration 323 0.08690034094948536\n",
      "Loss on Iteration 324 0.0837063338159888\n",
      "Loss on Iteration 325 0.08063057446169813\n",
      "Loss on Iteration 326 0.07766807304271947\n",
      "Loss on Iteration 327 0.07481477818870025\n",
      "Loss on Iteration 328 0.07206726186943294\n",
      "Loss on Iteration 329 0.06942085081694885\n",
      "Loss on Iteration 330 0.06687188015028503\n",
      "Loss on Iteration 331 0.06441739756400032\n",
      "Loss on Iteration 332 0.06205308919866992\n",
      "Loss on Iteration 333 0.059775886894574456\n",
      "Loss on Iteration 334 0.05758322525293179\n",
      "Loss on Iteration 335 0.05547083530170803\n",
      "Loss on Iteration 336 0.05343620508148586\n",
      "Loss on Iteration 337 0.05147693932595898\n",
      "Loss on Iteration 338 0.049589452044902005\n",
      "Loss on Iteration 339 0.04777155797184397\n",
      "Loss on Iteration 340 0.04602082404265741\n",
      "Loss on Iteration 341 0.0443341966431342\n",
      "Loss on Iteration 342 0.04270975229847884\n",
      "Loss on Iteration 343 0.0411453505925604\n",
      "Loss on Iteration 344 0.03963821610957601\n",
      "Loss on Iteration 345 0.038186557842728154\n",
      "Loss on Iteration 346 0.03678837908879858\n",
      "Loss on Iteration 347 0.035441464367843234\n",
      "Loss on Iteration 348 0.03414428229835426\n",
      "Loss on Iteration 349 0.032894855781086196\n",
      "Loss on Iteration 350 0.031691114251994404\n",
      "Loss on Iteration 351 0.030531618463519563\n",
      "Loss on Iteration 352 0.029414647338937012\n",
      "Loss on Iteration 353 0.028338616570411555\n",
      "Loss on Iteration 354 0.027302295659671193\n",
      "Loss on Iteration 355 0.02630387546168679\n",
      "Loss on Iteration 356 0.025342065037648284\n",
      "Loss on Iteration 357 0.02441575474924318\n",
      "Loss on Iteration 358 0.023523244704238236\n",
      "Loss on Iteration 359 0.02266349266146453\n",
      "Loss on Iteration 360 0.02183540218858108\n",
      "Loss on Iteration 361 0.021037566007127556\n",
      "Loss on Iteration 362 0.020269024118981075\n",
      "Loss on Iteration 363 0.019528739330946114\n",
      "Loss on Iteration 364 0.018815533190690844\n",
      "Loss on Iteration 365 0.018128483721495166\n",
      "Loss on Iteration 366 0.01746658181573179\n",
      "Loss on Iteration 367 0.01682887822621214\n",
      "Loss on Iteration 368 0.0162146402730498\n",
      "Loss on Iteration 369 0.015622818082865279\n",
      "Loss on Iteration 370 0.015052654218697337\n",
      "Loss on Iteration 371 0.014503467672340509\n",
      "Loss on Iteration 372 0.013974303325160819\n",
      "Loss on Iteration 373 0.013464527443898963\n",
      "Loss on Iteration 374 0.012973445354250754\n",
      "Loss on Iteration 375 0.012500263451265153\n",
      "Loss on Iteration 376 0.012044508065423691\n",
      "Loss on Iteration 377 0.011605369198841177\n",
      "Loss on Iteration 378 0.011182252569428718\n",
      "Loss on Iteration 379 0.01077471221475762\n",
      "Loss on Iteration 380 0.010381979145805732\n",
      "Loss on Iteration 381 0.01000360856552401\n",
      "Loss on Iteration 382 0.009639124798153313\n",
      "Loss on Iteration 383 0.009287911273657214\n",
      "Loss on Iteration 384 0.008949557467050628\n",
      "Loss on Iteration 385 0.00862359661520928\n",
      "Loss on Iteration 386 0.008309487959407523\n",
      "Loss on Iteration 387 0.008006907630216446\n",
      "Loss on Iteration 388 0.007715337038217989\n",
      "Loss on Iteration 389 0.0074344195114739425\n",
      "Loss on Iteration 390 0.007163798272400499\n",
      "Loss on Iteration 391 0.006903008588191321\n",
      "Loss on Iteration 392 0.00665176422119514\n",
      "Loss on Iteration 393 0.006409695968970466\n",
      "Loss on Iteration 394 0.006176444585912353\n",
      "Loss on Iteration 395 0.005951754185087119\n",
      "Loss on Iteration 396 0.0057352140816892706\n",
      "Loss on Iteration 397 0.005526615577702293\n",
      "Loss on Iteration 398 0.005325618810209971\n",
      "Loss on Iteration 399 0.005131913871049373\n",
      "Loss on Iteration 400 0.004945300359321562\n",
      "Loss on Iteration 401 0.004765488466000538\n",
      "Loss on Iteration 402 0.004592213709234661\n",
      "Loss on Iteration 403 0.0044252908237453585\n",
      "Loss on Iteration 404 0.004264439308775454\n",
      "Loss on Iteration 405 0.004109445486359508\n",
      "Loss on Iteration 406 0.0039601092363259136\n",
      "Loss on Iteration 407 0.0038162034935420182\n",
      "Loss on Iteration 408 0.003677556481409326\n",
      "Loss on Iteration 409 0.003543943817934504\n",
      "Loss on Iteration 410 0.00341519533265674\n",
      "Loss on Iteration 411 0.0032911634508026163\n",
      "Loss on Iteration 412 0.003171621281291761\n",
      "Loss on Iteration 413 0.0030564448112153884\n",
      "Loss on Iteration 414 0.0029454559707334854\n",
      "Loss on Iteration 415 0.0028385004395443543\n",
      "Loss on Iteration 416 0.0027354595857171067\n",
      "Loss on Iteration 417 0.0026361483078327064\n",
      "Loss on Iteration 418 0.0025404622237525454\n",
      "Loss on Iteration 419 0.002448258737815083\n",
      "Loss on Iteration 420 0.002359400457722558\n",
      "Loss on Iteration 421 0.0022737913836915523\n",
      "Loss on Iteration 422 0.0021912806197049556\n",
      "Loss on Iteration 423 0.002111780037751088\n",
      "Loss on Iteration 424 0.0020351721344003546\n",
      "Loss on Iteration 425 0.0019613407496826872\n",
      "Loss on Iteration 426 0.0018902079809783718\n",
      "Loss on Iteration 427 0.001821654094225501\n",
      "Loss on Iteration 428 0.0017555932294996367\n",
      "Loss on Iteration 429 0.001691936536558289\n",
      "Loss on Iteration 430 0.0016305904509412768\n",
      "Loss on Iteration 431 0.0015714828370397443\n",
      "Loss on Iteration 432 0.0015145147692352695\n",
      "Loss on Iteration 433 0.001459616666751694\n",
      "Loss on Iteration 434 0.001406714984172659\n",
      "Loss on Iteration 435 0.0013557295042766209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Iteration 436 0.0013066055218579112\n",
      "Loss on Iteration 437 0.0012592564844486891\n",
      "Loss on Iteration 438 0.0012136318512716252\n",
      "Loss on Iteration 439 0.0011696634115563546\n",
      "Loss on Iteration 440 0.0011272879215790846\n",
      "Loss on Iteration 441 0.0010864604127618664\n",
      "Loss on Iteration 442 0.00104710557821646\n",
      "Loss on Iteration 443 0.0010091847994379431\n",
      "Loss on Iteration 444 0.0009726406832543066\n",
      "Loss on Iteration 445 0.0009374203257983307\n",
      "Loss on Iteration 446 0.0009034864595252935\n",
      "Loss on Iteration 447 0.0008707737994286898\n",
      "Loss on Iteration 448 0.0008392523819449335\n",
      "Loss on Iteration 449 0.0008088721383951614\n",
      "Loss on Iteration 450 0.0007795933297905723\n",
      "Loss on Iteration 451 0.000751379751464007\n",
      "Loss on Iteration 452 0.0007241855840494913\n",
      "Loss on Iteration 453 0.0006979824807395415\n",
      "Loss on Iteration 454 0.0006727273763116664\n",
      "Loss on Iteration 455 0.0006483876546633812\n",
      "Loss on Iteration 456 0.0006249311564561256\n",
      "Loss on Iteration 457 0.000602322495041653\n",
      "Loss on Iteration 458 0.0005805382296744621\n",
      "Loss on Iteration 459 0.0005595409362235206\n",
      "Loss on Iteration 460 0.0005393059719648126\n",
      "Loss on Iteration 461 0.0005198031645416979\n",
      "Loss on Iteration 462 0.0005010059200264586\n",
      "Loss on Iteration 463 0.0004828919790542973\n",
      "Loss on Iteration 464 0.00046543124609674165\n",
      "Loss on Iteration 465 0.00044860659334666005\n",
      "Loss on Iteration 466 0.0004323885037685371\n",
      "Loss on Iteration 467 0.0004167593383541643\n",
      "Loss on Iteration 468 0.0004016960419383977\n",
      "Loss on Iteration 469 0.0003871772782700529\n",
      "Loss on Iteration 470 0.0003731864105587235\n",
      "Loss on Iteration 471 0.0003596999683351697\n",
      "Loss on Iteration 472 0.00034670374092562746\n",
      "Loss on Iteration 473 0.00033417637739532874\n",
      "Loss on Iteration 474 0.0003221035167328906\n",
      "Loss on Iteration 475 0.00031046752290980295\n",
      "Loss on Iteration 476 0.00029925184857129716\n",
      "Loss on Iteration 477 0.0002884443496859264\n",
      "Loss on Iteration 478 0.00027802643784672975\n",
      "Loss on Iteration 479 0.0002679867530025296\n",
      "Loss on Iteration 480 0.0002583086134653178\n",
      "Loss on Iteration 481 0.00024898131321081645\n",
      "Loss on Iteration 482 0.00023999139165362495\n",
      "Loss on Iteration 483 0.00023132624432579567\n",
      "Loss on Iteration 484 0.00022297552272282836\n",
      "Loss on Iteration 485 0.00021492639743622466\n",
      "Loss on Iteration 486 0.00020716892072655222\n",
      "Loss on Iteration 487 0.00019969069215537323\n",
      "Loss on Iteration 488 0.00019248370468931582\n",
      "Loss on Iteration 489 0.0001855371345644528\n",
      "Loss on Iteration 490 0.00017884145300555522\n",
      "Loss on Iteration 491 0.0001723883701033394\n",
      "Loss on Iteration 492 0.0001661677352798747\n",
      "Loss on Iteration 493 0.00016017310877178677\n",
      "Loss on Iteration 494 0.00015439406459033333\n",
      "Loss on Iteration 495 0.00014882513733122056\n",
      "Loss on Iteration 496 0.00014345650600403715\n",
      "Loss on Iteration 497 0.00013828283828330946\n",
      "Loss on Iteration 498 0.00013329537108208405\n",
      "Loss on Iteration 499 0.0001284878205090281\n"
     ]
    }
   ],
   "source": [
    "# Neural Network Using Numpy\n",
    "\n",
    "#Create dimensions\n",
    "batch_size = 64\n",
    "input_size = 1000\n",
    "hidden_size = 100\n",
    "output_size = 10\n",
    "\n",
    "#Initialize x & y\n",
    "x = np.random.randn(batch_size, input_size)\n",
    "y = np.random.randn(batch_size, output_size)\n",
    "\n",
    "#Initialize weights for x & y\n",
    "w1 = np.random.randn(input_size, hidden_size)\n",
    "w2 = np.random.randn(hidden_size, output_size)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for i in range(500):\n",
    "    #Predict y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    #Loss\n",
    "    loss = np.square(y-y_pred).sum()\n",
    "    print('Loss on Iteration', i, loss)\n",
    "    \n",
    "    #Backpropogation to update weighted values\n",
    "    #Compute gradients of weights with respect to loss to minimize cost\n",
    "    grad_y_pred = 2*(y_pred-y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0]=0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    w1 -= learning_rate*grad_w1\n",
    "    w2 -= learning_rate*grad_w2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network using Pytorch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "\n",
    "batch_size = 50\n",
    "input_size = 1000\n",
    "hidden_size = 100\n",
    "output_size = 10\n",
    "\n",
    "#Initialize x & y\n",
    "x = torch.randn(batch_size, input_size, dtype = dtype, device = device)\n",
    "y = torch.randn(batch_size, output_size, dtype = dtype, device = device)\n",
    "\n",
    "#Initialize weights\n",
    "w1 = torch.randn(input_size, hidden_size, dtype = dtype, device = device)\n",
    "w2 = torch.randn(hidden_size, output_size, dtype = dtype, device = device)\n",
    "\n",
    "# Find predictions of y\n",
    "for i in range(500):\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    #Compute Loss\n",
    "    loss = (y-y_pred).pow(2).sum()\n",
    "    print(i, loss.item())\n",
    "    \n",
    "    #Backpropogation\n",
    "    grad_y_pred = 2*(y_pred-y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h<0]=0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    #Update weights\n",
    "    w1 -= learning_rate*grad_w1\n",
    "    w2 -= learning_rate*grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
